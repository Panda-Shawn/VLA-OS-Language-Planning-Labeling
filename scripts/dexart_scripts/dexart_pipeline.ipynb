{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert tf record to png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bucket as example\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import imageio.v2 as imageio  \n",
    "\n",
    "from utils import DATA_DIR\n",
    "\n",
    "tfrecord_dir = DATA_DIR / \"tfds_datasets/bucket_dex_art_dataset/1.0.0\"\n",
    "tfrecord_dir = str(tfrecord_dir)\n",
    "\n",
    "base_save_dir = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/dexart_all_bucket_png\"\n",
    "base_save_dir = str(base_save_dir)\n",
    "camera_views = [\n",
    "    \"bucket_viz\"\n",
    "]\n",
    "\n",
    "os.makedirs(base_save_dir, exist_ok=True)\n",
    "\n",
    "tfrecord_files = sorted([\n",
    "    os.path.join(tfrecord_dir, f)\n",
    "    for f in os.listdir(tfrecord_dir)\n",
    "    if f.startswith(\"dex_art_dataset-train.tfrecord\")\n",
    "])\n",
    "\n",
    "for view in camera_views:\n",
    "    print(f\"\\nstart export: {view}\")\n",
    "    save_dir = os.path.join(base_save_dir, view)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    frame_counts = {}\n",
    "\n",
    "    def parse_example(example_proto):\n",
    "        feature_description = {\n",
    "            f\"steps/observation/{view}\": tf.io.VarLenFeature(tf.string),\n",
    "            \"episode_metadata/file_path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    for tfrecord_path in tqdm(tfrecord_files, desc=f\"处理 TFRecord - {view}\"):\n",
    "        raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "        for raw_record in raw_dataset:\n",
    "            example = parse_example(raw_record)\n",
    "            episode_path = example[\"episode_metadata/file_path\"].numpy().decode()\n",
    "            episode_id = os.path.splitext(os.path.basename(episode_path))[0]\n",
    "\n",
    "            try:\n",
    "                frames = tf.sparse.to_dense(example[f\"steps/observation/{view}\"]).numpy()\n",
    "            except:\n",
    "                print(f\"[skip] {episode_id} missing {view}\")\n",
    "                continue\n",
    "\n",
    "            if len(frames) == 0:\n",
    "                continue\n",
    "\n",
    "            frame_counts[episode_id] = len(frames)\n",
    "            print(f\"[frame count] {episode_id} has {len(frames)} frames\")\n",
    "\n",
    "            episode_dir = os.path.join(save_dir, episode_id)\n",
    "            os.makedirs(episode_dir, exist_ok=True)\n",
    "\n",
    "            for i, img_bytes in enumerate(frames):\n",
    "                try:\n",
    "                    img = tf.io.decode_png(img_bytes).numpy()\n",
    "                    if img.shape[-1] == 4:\n",
    "                        img = img[:, :, :3]  # 去除 alpha 通道\n",
    "                    frame_path = os.path.join(episode_dir, f\"frame_{i:03d}.png\")\n",
    "                    imageio.imwrite(frame_path, img)\n",
    "                except Exception as e:\n",
    "                    print(f\"[skip frame] {episode_id} frame {i} decode failed: {e}\")\n",
    "\n",
    "    frame_values = list(frame_counts.values())\n",
    "    if frame_values:\n",
    "        print(f\"\\nframe count statistics - {view}\")\n",
    "        print(f\"total video count: {len(frame_values)}\")\n",
    "        print(f\"average frame count: {np.mean(frame_values):.2f}\")\n",
    "        print(f\"max frame count: {np.max(frame_values)}\")\n",
    "        print(f\"min frame count: {np.min(frame_values)}\")\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"episode_id\": list(frame_counts.keys()),\n",
    "            \"frame_count\": frame_values\n",
    "        })\n",
    "        csv_path = os.path.join(save_dir, f\"{view}_frame_counts.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"frame count statistics saved to: {csv_path}\")\n",
    "    else:\n",
    "        print(f\"no valid frame data found - {view}\")\n",
    "\n",
    "print(\"\\nfinished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gripper position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  gpos for bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import transforms3d\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import imageio.v2 as imageio\n",
    "import cv2\n",
    "\n",
    "from utils import DATA_DIR\n",
    "\n",
    "tfrecord_dir = DATA_DIR / \"tfds_datasets/bucket_dex_art_dataset/1.0.0\"\n",
    "tfrecord_dir = str(tfrecord_dir)\n",
    "\n",
    "video_dir = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/dexart_all_bucket_png/bucket_viz\"\n",
    "video_dir = str(video_dir)\n",
    "save_video_dir = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/bucket/vis_gpos\"\n",
    "save_video_dir = str(save_video_dir)\n",
    "save_json_path = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/bucket/gripper_positions.json\"\n",
    "save_json_path = str(save_json_path)\n",
    "\n",
    "TARGET_RES = 224\n",
    "CAMERA_RES = 1000\n",
    "SCALE = TARGET_RES / CAMERA_RES\n",
    "\n",
    "\n",
    "os.makedirs(save_video_dir, exist_ok=True)\n",
    "for filename in os.listdir(save_video_dir):\n",
    "    file_path = os.path.join(save_video_dir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(f'delete {file_path} failed: {e}')\n",
    "\n",
    "def get_bucket_extrinsics():\n",
    "    position = np.array([0, 1, 0.5])\n",
    "    quat = transforms3d.euler.euler2quat(np.pi / 2, np.pi, 0)\n",
    "    R = transforms3d.quaternions.quat2mat(quat)\n",
    "    t = position\n",
    "    return R, t\n",
    "\n",
    "def get_bucket_intrinsics(fov_deg=69.4):\n",
    "    fov_rad = np.deg2rad(fov_deg)\n",
    "    fx = fy = CAMERA_RES / (2 * np.tan(fov_rad / 2))\n",
    "    cx = cy = CAMERA_RES / 2\n",
    "    return np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "\n",
    "def project_gripper_to_image(pos_world, K, R, t):\n",
    "    x_cam = R @ (pos_world - t)\n",
    "    x, y, z = x_cam\n",
    "    if z <= 0:\n",
    "        return [-1, -1]\n",
    "    u = (K[0, 0] * x / z) + K[0, 2]\n",
    "    v = (K[1, 1] * y / z) + K[1, 2]\n",
    "    return [int(round(u)), int(round(v))]\n",
    "\n",
    "def parse_single_example(example_proto):\n",
    "    feature_description = {\n",
    "        \"episode_metadata/file_path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"steps/state\": tf.io.VarLenFeature(tf.float32),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "def run():\n",
    "    K = get_bucket_intrinsics()\n",
    "    R, t = get_bucket_extrinsics()\n",
    "    uv_scaled_dict = {}\n",
    "\n",
    "    tfrecord_files = sorted([\n",
    "        os.path.join(tfrecord_dir, f)\n",
    "        for f in os.listdir(tfrecord_dir)\n",
    "        if f.endswith(\".tfrecord\") or \".tfrecord-\" in f\n",
    "    ])\n",
    "\n",
    "    for tf_file in tqdm(tfrecord_files, desc=\"processing tfrecords\"):\n",
    "        dataset = tf.data.TFRecordDataset(tf_file)\n",
    "        for raw_record in dataset:\n",
    "            example = parse_single_example(raw_record)\n",
    "            file_path = example[\"episode_metadata/file_path\"].numpy().decode()\n",
    "            state_seq = tf.sparse.to_dense(example[\"steps/state\"]).numpy()\n",
    "\n",
    "            if len(state_seq) == 0:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                states = state_seq.reshape(-1, 33)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            gripper_pos_seq = states[:, 28:31]\n",
    "            uv_full = [project_gripper_to_image(p, K, R, t) for p in gripper_pos_seq]\n",
    "            uv_scaled = [[int(u * SCALE), int(v * SCALE)] if u >= 0 and v >= 0 else [-1, -1] for u, v in uv_full]\n",
    "\n",
    "            match = re.search(r\"episode_(\\d+)_combined\\.npz$\", file_path)\n",
    "            if not match:\n",
    "                continue\n",
    "            episode_id = match.group(1)\n",
    "            uv_scaled_dict[episode_id] = uv_scaled\n",
    "\n",
    "            frame_dir = os.path.join(video_dir, f\"episode_{episode_id}_combined\")\n",
    "\n",
    "            if not os.path.exists(frame_dir):\n",
    "                print(f\"[skip] missing frame folder: {frame_dir}\")\n",
    "                continue\n",
    "\n",
    "            frame_paths = sorted(glob.glob(os.path.join(frame_dir, \"frame_*.png\")))\n",
    "            if len(frame_paths) == 0:\n",
    "                print(f\"[skip] no image frames: {frame_dir}\")\n",
    "                continue\n",
    "\n",
    "            fps = 10\n",
    "            frame_count = len(frame_paths)\n",
    "            print(f\"[align check] {episode_id}: state frame count={states.shape[0]}, image frame count={frame_count}\")\n",
    "\n",
    "            out_path = os.path.join(save_video_dir, f\"{episode_id}_vis.mp4\")\n",
    "            frame0 = imageio.imread(frame_paths[0])\n",
    "            h, w = TARGET_RES, TARGET_RES\n",
    "            out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "            for frame_idx, frame_path in enumerate(frame_paths):\n",
    "                if frame_idx >= len(uv_scaled):\n",
    "                    break\n",
    "\n",
    "                frame = imageio.imread(frame_path)\n",
    "                frame = cv2.resize(frame, (TARGET_RES, TARGET_RES))\n",
    "\n",
    "                u, v = uv_scaled[frame_idx]\n",
    "                u = max(0, min(u, TARGET_RES - 1))\n",
    "                v = max(0, min(v, TARGET_RES - 1))\n",
    "                cv2.circle(frame, (u, v), radius=4, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "                pos_xyz = gripper_pos_seq[frame_idx]\n",
    "                text = f\"x={pos_xyz[0]:.3f}, y={pos_xyz[1]:.3f}, z={pos_xyz[2]:.3f}\"\n",
    "                cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n",
    "\n",
    "                origin_uv = project_gripper_to_image(np.array([0, 0, 0]), K, R, t)\n",
    "                if origin_uv[0] >= 0 and origin_uv[1] >= 0:\n",
    "                    origin_uv_scaled = [int(origin_uv[0] * SCALE), int(origin_uv[1] * SCALE)]\n",
    "                    cv2.circle(frame, tuple(origin_uv_scaled), radius=4, color=(0, 255, 0), thickness=-1)\n",
    "                    cv2.putText(frame, \"origin\", (origin_uv_scaled[0] + 5, origin_uv_scaled[1] - 5),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)\n",
    "\n",
    "                out.write(frame)\n",
    "\n",
    "            out.release()\n",
    "            print(f\"[save video] {out_path}\")\n",
    "\n",
    "    with open(save_json_path, \"w\") as f:\n",
    "        json.dump(uv_scaled_dict, f, indent=2)\n",
    "    print(f\"[save json] {save_json_path}\")\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) If you are just running the bucket example, please skip this cell. Run it only when you need the gripper_position for the other three DexArt objects, as their data dimensions differ from those of the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import transforms3d\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import imageio.v2 as imageio\n",
    "import cv2\n",
    "\n",
    "from utils import DATA_DIR\n",
    "\n",
    "tfrecord_dir = DATA_DIR / \"tfds_datasets/bucket_dex_art_dataset/1.0.0\"\n",
    "tfrecord_dir = str(tfrecord_dir)\n",
    "\n",
    "video_dir = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/dexart_all_bucket_png/bucket_viz\"\n",
    "video_dir = str(video_dir)\n",
    "save_video_dir = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/results_gpos/bucket/vis_gpos\"\n",
    "save_video_dir = str(save_video_dir)\n",
    "save_json_path = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/results_gpos/bucket/gripper_positions.json\"\n",
    "save_json_path = str(save_json_path)\n",
    "\n",
    "TARGET_RES = 224\n",
    "CAMERA_RES = 1000\n",
    "SCALE = TARGET_RES / CAMERA_RES\n",
    "\n",
    "os.makedirs(save_video_dir, exist_ok=True)\n",
    "for filename in os.listdir(save_video_dir):\n",
    "    file_path = os.path.join(save_video_dir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(f'delete {file_path} failed: {e}')\n",
    "\n",
    "\n",
    "\n",
    "def get_bucket_extrinsics():\n",
    "    # === faucet_viz2  ===\n",
    "    position = np.array([0, 0.8, 0.5])\n",
    "    quat = transforms3d.euler.euler2quat(np.pi / 3, np.pi, 0)  \n",
    "\n",
    "    R_cam_to_world = transforms3d.quaternions.quat2mat(quat)\n",
    "\n",
    "    R_world_to_cam = R_cam_to_world.T\n",
    "    t = position  \n",
    "    return R_world_to_cam, t\n",
    "\n",
    "\n",
    "def get_bucket_intrinsics(fov_deg=69.4):\n",
    "    fov_rad = np.deg2rad(fov_deg)\n",
    "    fx = fy = CAMERA_RES / (2 * np.tan(fov_rad / 2))\n",
    "    cx = cy = CAMERA_RES / 2\n",
    "    return np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "\n",
    "def project_gripper_to_image(pos_world, K, R, t):\n",
    "    x_cam = R @ (pos_world - t)\n",
    "    x, y, z = x_cam\n",
    "    if z <= 0:\n",
    "        return [-1, -1]\n",
    "    u = (K[0, 0] * x / z) + K[0, 2]\n",
    "    v = (K[1, 1] * y / z) + K[1, 2]\n",
    "    return [int(round(u)), int(round(v))]\n",
    "\n",
    "def parse_single_example(example_proto):\n",
    "    feature_description = {\n",
    "        \"episode_metadata/file_path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"steps/state\": tf.io.VarLenFeature(tf.float32),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "def run():\n",
    "    K = get_bucket_intrinsics()\n",
    "    R, t = get_bucket_extrinsics()\n",
    "    uv_scaled_dict = {}\n",
    "\n",
    "    tfrecord_files = sorted([\n",
    "        os.path.join(tfrecord_dir, f)\n",
    "        for f in os.listdir(tfrecord_dir)\n",
    "        if f.endswith(\".tfrecord\") or \".tfrecord-\" in f\n",
    "    ])\n",
    "\n",
    "    for tf_file in tqdm(tfrecord_files, desc=\"处理TFRecords\"):\n",
    "        dataset = tf.data.TFRecordDataset(tf_file)\n",
    "        for raw_record in dataset:\n",
    "            example = parse_single_example(raw_record)\n",
    "            file_path = example[\"episode_metadata/file_path\"].numpy().decode()\n",
    "            state_seq = tf.sparse.to_dense(example[\"steps/state\"]).numpy()\n",
    "\n",
    "            if len(state_seq) == 0:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                states = state_seq.reshape(-1, 32)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            gripper_pos_seq = states[:, -4:-1]\n",
    "            uv_full = [project_gripper_to_image(p, K, R, t) for p in gripper_pos_seq]\n",
    "            uv_scaled = [[int(u * SCALE), int(v * SCALE)] if u >= 0 and v >= 0 else [-1, -1] for u, v in uv_full]\n",
    "\n",
    "            match = re.search(r\"episode_(\\d+)_combined\\.npz$\", file_path)\n",
    "            if not match:\n",
    "                continue\n",
    "            episode_id = match.group(1)\n",
    "            uv_scaled_dict[episode_id] = uv_scaled\n",
    "\n",
    "            frame_dir = os.path.join(video_dir, f\"episode_{episode_id}_combined\")\n",
    "\n",
    "            if not os.path.exists(frame_dir):\n",
    "                print(f\"[skip] missing frame folder: {frame_dir}\")\n",
    "                continue\n",
    "\n",
    "            frame_paths = sorted(glob.glob(os.path.join(frame_dir, \"frame_*.png\")))\n",
    "            if len(frame_paths) == 0:\n",
    "                print(f\"[skip] no image frames: {frame_dir}\")\n",
    "                continue\n",
    "\n",
    "            fps = 10\n",
    "            frame_count = len(frame_paths)\n",
    "            print(f\"[align check] {episode_id}: state frame count={states.shape[0]}, image frame count={frame_count}\")\n",
    "\n",
    "            out_path = os.path.join(save_video_dir, f\"{episode_id}_vis.mp4\")\n",
    "            frame0 = imageio.imread(frame_paths[0])\n",
    "            h, w = TARGET_RES, TARGET_RES\n",
    "            out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "            for frame_idx, frame_path in enumerate(frame_paths):\n",
    "                if frame_idx >= len(uv_scaled):\n",
    "                    break\n",
    "\n",
    "                frame = imageio.imread(frame_path)\n",
    "                frame = cv2.resize(frame, (TARGET_RES, TARGET_RES))\n",
    "\n",
    "                u, v = uv_scaled[frame_idx]\n",
    "                u = max(0, min(u, TARGET_RES - 1))\n",
    "                v = max(0, min(v, TARGET_RES - 1))\n",
    "                cv2.circle(frame, (u, v), radius=4, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "                pos_xyz = gripper_pos_seq[frame_idx]\n",
    "                text = f\"x={pos_xyz[0]:.3f}, y={pos_xyz[1]:.3f}, z={pos_xyz[2]:.3f}\"\n",
    "                cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n",
    "\n",
    "                origin_uv = project_gripper_to_image(np.array([0, 0, 0]), K, R, t)\n",
    "                if origin_uv[0] >= 0 and origin_uv[1] >= 0:\n",
    "                    origin_uv_scaled = [int(origin_uv[0] * SCALE), int(origin_uv[1] * SCALE)]\n",
    "                    cv2.circle(frame, tuple(origin_uv_scaled), radius=4, color=(0, 255, 0), thickness=-1)\n",
    "                    cv2.putText(frame, \"origin\", (origin_uv_scaled[0] + 5, origin_uv_scaled[1] - 5),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)\n",
    "\n",
    "                out.write(frame)\n",
    "\n",
    "            out.release()\n",
    "            print(f\"[save video] {out_path}\")\n",
    "\n",
    "    with open(save_json_path, \"w\") as f:\n",
    "        json.dump(uv_scaled_dict, f, indent=2)\n",
    "    print(f\"[save json] {save_json_path}\")\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move primitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### move primitive for bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# ==== 配置路径 ====\n",
    "from utils import DATA_DIR\n",
    "\n",
    "tfrecord_dir = DATA_DIR / \"tfds_datasets/bucket_dex_art_dataset/1.0.0\"\n",
    "tfrecord_dir = str(tfrecord_dir)\n",
    "\n",
    "save_dir = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/results_move_primitives/bucket\"\n",
    "save_dir = str(save_dir)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "tfrecord_files = sorted([\n",
    "    os.path.join(tfrecord_dir, f)\n",
    "    for f in os.listdir(tfrecord_dir)\n",
    "    if f.startswith(\"dex_art_dataset-train.tfrecord\")\n",
    "])\n",
    "\n",
    "def describe_move(move_vec):\n",
    "    direction_names = [\n",
    "        {-1: \"backward\", 0: None, 1: \"forward\"},   # x\n",
    "        {-1: \"right\",    0: None, 1: \"left\"},      # y\n",
    "        {-1: \"down\",     0: None, 1: \"up\"},        # z\n",
    "    ]\n",
    "\n",
    "    move_descriptions = [direction_names[i][move_vec[i]] for i in range(3)]\n",
    "    move_descriptions = [desc for desc in move_descriptions if desc is not None]\n",
    "\n",
    "    if len(move_descriptions) == 0:\n",
    "        return \"stop\"\n",
    "    else:\n",
    "        return \"move \" + \" \".join(move_descriptions)\n",
    "\n",
    "\n",
    "def classify_movement(move, threshold=0.003):\n",
    "    diff = move[-1] - move[0]\n",
    "\n",
    "    if np.sum(np.abs(diff[:3])) > 3 * threshold:\n",
    "        diff[:3] *= 3 * threshold / np.sum(np.abs(diff[:3]))\n",
    "\n",
    "    move_vec = 1 * (diff[:3] > threshold) - 1 * (diff[:3] < -threshold)\n",
    "    return describe_move(move_vec), move_vec\n",
    "\n",
    "\n",
    "def get_move_primitives_from_states(states):\n",
    "    move_trajs = [states[i : i + 4] for i in range(len(states) - 1)]\n",
    "    primitives = [classify_movement(move) for move in move_trajs]\n",
    "    primitives.append(primitives[-1])  # 补上最后一帧\n",
    "    return primitives\n",
    "\n",
    "def parse_single_example(example_proto):\n",
    "    feature_description = {\n",
    "        \"episode_metadata/file_path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"steps/state\": tf.io.VarLenFeature(tf.float32),  # 注意：必须使用 float32，读取后转为 float64\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "all_move_texts = {}\n",
    "all_states_list = []\n",
    "\n",
    "for tfrecord_path in tqdm(tfrecord_files, desc=\"reading tfrecords\"):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "    for raw_record in raw_dataset:\n",
    "        try:\n",
    "            example = parse_single_example(raw_record)\n",
    "\n",
    "            file_path = example[\"episode_metadata/file_path\"].numpy().decode()\n",
    "            save_key = file_path\n",
    "\n",
    "            state_seq = tf.sparse.to_dense(example[\"steps/state\"]).numpy().astype(np.float64)\n",
    "\n",
    "            if len(state_seq) == 0:\n",
    "                print(f\"[skip] {save_key} no state\")\n",
    "                continue\n",
    "\n",
    "            states = state_seq.reshape(-1, 32)  \n",
    "            states_for_move = states[:, 28:31]     \n",
    "\n",
    "            move_primitives = get_move_primitives_from_states(states_for_move)\n",
    "            move_text_list = [move_text for move_text, move_vec in move_primitives]\n",
    "\n",
    "            all_move_texts[save_key] = move_text_list\n",
    "            all_states_list.append(states)\n",
    "\n",
    "            print(f\"[done] {save_key}: {states.shape[0]} frames\")\n",
    "        except Exception as e:\n",
    "            print(f\"[error] {tfrecord_path}: {e}\")\n",
    "\n",
    "# ==== 保存 ====\n",
    "all_states_concat = np.concatenate(all_states_list, axis=0)\n",
    "np.save(os.path.join(save_dir, \"all_states.npy\"), all_states_concat)\n",
    "\n",
    "with open(os.path.join(save_dir, \"all_moves_list.json\"), \"w\") as f:\n",
    "    json.dump(all_move_texts, f, indent=2)\n",
    "\n",
    "print(\"finished（file_path -> move list）\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### move primitive for other three dexart objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# ==== 配置路径 ====\n",
    "from utils import DATA_DIR\n",
    "\n",
    "tfrecord_dir = DATA_DIR / \"tfds_datasets/bucket_dex_art_dataset/1.0.0\"\n",
    "tfrecord_dir = str(tfrecord_dir)\n",
    "save_dir = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/results_move_primitives/bucket\"\n",
    "save_dir = str(save_dir)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "tfrecord_files = sorted([\n",
    "    os.path.join(tfrecord_dir, f)\n",
    "    for f in os.listdir(tfrecord_dir)\n",
    "    if f.startswith(\"dex_art_dataset-train.tfrecord\")\n",
    "])\n",
    "\n",
    "def describe_move(move_vec):\n",
    "    direction_names = [\n",
    "        {-1: \"backward\", 0: None, 1: \"forward\"},   # x\n",
    "        {-1: \"right\",    0: None, 1: \"left\"},      # y\n",
    "        {-1: \"down\",     0: None, 1: \"up\"},        # z\n",
    "    ]\n",
    "\n",
    "    move_descriptions = [direction_names[i][move_vec[i]] for i in range(3)]\n",
    "    move_descriptions = [desc for desc in move_descriptions if desc is not None]\n",
    "\n",
    "    if len(move_descriptions) == 0:\n",
    "        return \"stop\"\n",
    "    else:\n",
    "        return \"move \" + \" \".join(move_descriptions)\n",
    "\n",
    "\n",
    "def classify_movement(move, threshold=0.003):\n",
    "    diff = move[-1] - move[0]\n",
    "\n",
    "    if np.sum(np.abs(diff[:3])) > 3 * threshold:\n",
    "        diff[:3] *= 3 * threshold / np.sum(np.abs(diff[:3]))\n",
    "\n",
    "    move_vec = 1 * (diff[:3] > threshold) - 1 * (diff[:3] < -threshold)\n",
    "    return describe_move(move_vec), move_vec\n",
    "\n",
    "\n",
    "def get_move_primitives_from_states(states):\n",
    "    move_trajs = [states[i : i + 4] for i in range(len(states) - 1)]\n",
    "    primitives = [classify_movement(move) for move in move_trajs]\n",
    "    primitives.append(primitives[-1])  # 补上最后一帧\n",
    "    return primitives\n",
    "\n",
    "def parse_single_example(example_proto):\n",
    "    feature_description = {\n",
    "        \"episode_metadata/file_path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"steps/state\": tf.io.VarLenFeature(tf.float32),  # 注意：必须使用 float32，读取后转为 float64\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "all_move_texts = {}\n",
    "all_states_list = []\n",
    "\n",
    "for tfrecord_path in tqdm(tfrecord_files, desc=\"reading tfrecords\"):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "\n",
    "    for raw_record in raw_dataset:\n",
    "        try:\n",
    "            example = parse_single_example(raw_record)\n",
    "\n",
    "            file_path = example[\"episode_metadata/file_path\"].numpy().decode()\n",
    "            save_key = file_path\n",
    "\n",
    "            state_seq = tf.sparse.to_dense(example[\"steps/state\"]).numpy().astype(np.float64)\n",
    "\n",
    "            if len(state_seq) == 0:\n",
    "                print(f\"[skip] {save_key} no state\")\n",
    "                continue\n",
    "\n",
    "            states = state_seq.reshape(-1, 32)  \n",
    "            states_for_move = states[:, -4:-1]     \n",
    "\n",
    "            move_primitives = get_move_primitives_from_states(states_for_move)\n",
    "            move_text_list = [move_text for move_text, move_vec in move_primitives]\n",
    "\n",
    "            all_move_texts[save_key] = move_text_list\n",
    "            all_states_list.append(states)\n",
    "\n",
    "            print(f\"[done] {save_key}: {states.shape[0]} frames\")\n",
    "        except Exception as e:\n",
    "            print(f\"[error] {tfrecord_path}: {e}\")\n",
    "\n",
    "all_states_concat = np.concatenate(all_states_list, axis=0)\n",
    "np.save(os.path.join(save_dir, \"all_states.npy\"), all_states_concat)\n",
    "\n",
    "with open(os.path.join(save_dir, \"all_moves_list.json\"), \"w\") as f:\n",
    "    json.dump(all_move_texts, f, indent=2)\n",
    "\n",
    "print(\"finished（file_path -> move list）\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post process for move primitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continue to process the extracted move primitive (construct the file name key and move to save to the data_middle folder, and then convert key to pure numbers)\n",
    "import json\n",
    "import os\n",
    "\n",
    "from utils import DATA_DIR\n",
    "\n",
    "# input psth\n",
    "INPUT_PATH = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/results_move_primitives/bucket/all_moves_list.json\"\n",
    "INPUT_PATH = str(INPUT_PATH)\n",
    "# MIDDLE data out put path\n",
    "OUTPUT_PATH = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/results_move_primitives/bucket/raw_primitives.json\"\n",
    "OUTPUT_PATH = str(OUTPUT_PATH)\n",
    "#FINAL data out put dir\n",
    "output_dir = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/results_move_primitives/bucket\"\n",
    "output_dir = str(output_dir)\n",
    "\n",
    "with open(INPUT_PATH, \"r\") as f:\n",
    "    raw_moves = json.load(f)\n",
    "\n",
    "formatted_moves = {}\n",
    "\n",
    "for episode_id_str, move_list in raw_moves.items():\n",
    "    #key：object_box_Task_{episode_id}_Demo_0\n",
    "    formatted_key = f\"{episode_id_str}\"\n",
    "    formatted_moves[formatted_key] = move_list\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    json.dump(formatted_moves, f)\n",
    "\n",
    "print(f\"Saved rawprimitives.json to {OUTPUT_PATH}\")\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "def convert_keys(input_path, output_path):\n",
    "    with open(input_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    converted_data = {}\n",
    "    for key, value in data.items():\n",
    "        # match /data2/zyx/demo_dataset/fold/88 → extract \"88\"\n",
    "        match = re.search(r\"episode_(\\d+)_combined\\.npz$\", key)\n",
    "\n",
    "\n",
    "        if match:\n",
    "            episode_id = match.group(1)\n",
    "            converted_data[episode_id] = value\n",
    "        else:\n",
    "            print(f\"[WARN] Key format not matched: {key}\")\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(converted_data, f, indent=2)\n",
    "    print(f\"Saved converted primitives file to {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "convert_keys(OUTPUT_PATH, f\"{output_dir}/primitives.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize move primitive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "obj ='bucket'\n",
    "\n",
    "from utils import DATA_DIR\n",
    "\n",
    "json_path = DATA_DIR / f\"planning_datasets/bucket_dex_art_dataset/results_move_primitives/{obj}/primitives.json\"\n",
    "json_path = str(json_path)\n",
    "image_root = DATA_DIR / f\"planning_datasets/bucket_dex_art_dataset/dexart_all_{obj}_png/{obj}_viz\"\n",
    "image_root = str(image_root)\n",
    "save_root = DATA_DIR / f\"planning_datasets/bucket_dex_art_dataset/results_move_primitives/a_check_move_gpos/{obj}\"\n",
    "save_root = str(save_root)\n",
    "os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    primitives = json.load(f)\n",
    "\n",
    "for episode_id, moves in tqdm(primitives.items()):\n",
    "    input_folder = os.path.join(image_root, f\"episode_{episode_id}_combined\")\n",
    "    output_folder = os.path.join(save_root, f\"episode_{episode_id}_combined\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    png_files = sorted(glob(os.path.join(input_folder, \"*.png\")))\n",
    "\n",
    "    if len(moves) != len(png_files):\n",
    "        print(f\"[Mismatch] Episode {episode_id}: {len(moves)} moves vs {len(png_files)} PNGs\")\n",
    "        continue\n",
    "\n",
    "    for png_path, move in zip(png_files, moves):\n",
    "        image = cv2.imread(png_path)\n",
    "\n",
    "        if image is None:\n",
    "            print(f\"[Error] Cannot read image: {png_path}\")\n",
    "            continue\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(image, move, (30, 50), font, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        filename = os.path.basename(png_path)\n",
    "        save_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(save_path, image)\n",
    "\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scense description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First run `dexart/scense_description/scripts/generate_descriptions_dexart.py` to generate description for object.\n",
    " \n",
    " Then run following cell to get formalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "obj='bucket'\n",
    "\n",
    "from utils import DATA_DIR\n",
    "\n",
    "input_path = DATA_DIR / f\"planning_datasets/bucket_dex_art_dataset/results_descriptions/{obj}/results_0.json\"\n",
    "input_path = str(input_path)\n",
    "output_path = DATA_DIR / f\"planning_datasets/bucket_dex_art_dataset/results_descriptions/{obj}/descriptions_{obj}_fixed.json\"\n",
    "output_path = str(output_path)\n",
    "\n",
    "with open(input_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "new_data = {}\n",
    "for k, v in data.items():\n",
    "    new_key = k.split(\"_\")[-3] \n",
    "    new_data[new_key] = v\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(new_data, f, indent=2)\n",
    "\n",
    "print(f\"finished, saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bounding box "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the environment from [Grounded-SAM-2](https://github.com/IDEA-Research/Grounded-SAM-2).\n",
    "2. Run `third_party/Grounded-SAM-2/point_object_by_hand.py` and point the target object to get anchor. \n",
    "3. Modify `INPUT_DIR` to the folder containing png extracted from tf.\n",
    "4. Run following scripts to get bounding box raw data. \n",
    "```bash\n",
    "cd third_party/Grounded-SAM-2\n",
    "conda activate grounded_sam2\n",
    "python bbox_dexart.py\n",
    "```\n",
    "5. run following cell to extract raw bounding box data to json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import DATA_DIR\n",
    "\n",
    "INPUT_DIR = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/results_bbox/bucket\"\n",
    "INPUT_DIR = str(INPUT_DIR)\n",
    "\n",
    "OUTPUT_PATH = f\"{INPUT_DIR}/bboxes.json\"\n",
    "\n",
    "TARGET_NAME = INPUT_DIR.split('/')[-1]\n",
    "\n",
    "bboxes_all = {}\n",
    "\n",
    "for episode_folder in tqdm(sorted(os.listdir(INPUT_DIR)), desc=\"Processing episodes\"):\n",
    "    episode_path = os.path.join(INPUT_DIR, episode_folder)\n",
    "    video_boxes_path = os.path.join(episode_path, \"video_boxes.json\")\n",
    "\n",
    "    if not os.path.isfile(video_boxes_path):\n",
    "        print(f\"Missing video_boxes.json in {episode_folder}, skipped.\")\n",
    "        continue\n",
    "\n",
    "    with open(video_boxes_path, \"r\") as f:\n",
    "        frame_box_data = json.load(f)\n",
    "\n",
    "    frame_keys = sorted(frame_box_data.keys(), key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "    episode_boxes = []\n",
    "\n",
    "    for frame_key in frame_keys:\n",
    "        objects = frame_box_data.get(frame_key, [])\n",
    "        frame_boxes = []\n",
    "\n",
    "        for obj in objects:\n",
    "            if isinstance(obj, list) and len(obj) == 2:\n",
    "                original_name, bbox = obj\n",
    "                frame_boxes.append([TARGET_NAME, bbox]) \n",
    "\n",
    "        episode_boxes.append([frame_boxes[0]] if frame_boxes else [])  \n",
    "\n",
    "    episode_id = episode_folder.replace(\"episode_\", \"\").replace(\"_combined\", \"\")\n",
    "    bboxes_all[episode_id] = episode_boxes\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    json.dump(bboxes_all, f)\n",
    "\n",
    "print(f\"Saved bboxes.json to {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run `dexart/CoT/cot_code/batch_generate_plan_subtasks.sh`.\n",
    "2. Run `dexart/CoT/cot_code/batch_filter_plan_subtasks.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name of gpos.json to gripper_positions.json if encounter error\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from utils import DATA_DIR\n",
    "\n",
    "INPUT_DIR = DATA_DIR / \"planning_datasets/bucket_dex_art_dataset/results_bbox/bucket\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--libero_task_suite\", type=str, default=\"bucket\")\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"/data/lyd/embodied-CoT/data_results/data_cot_results/data_middle/dexart/results_cot\")\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    bboxes_file_path = os.path.join(args.data_dir, args.libero_task_suite, f\"bboxes.json\")\n",
    "    with open(bboxes_file_path, \"r\") as f:\n",
    "        bboxes = json.load(f)\n",
    "\n",
    "    gripper_positions_file_path = os.path.join(args.data_dir, args.libero_task_suite, f\"gripper_positions.json\")\n",
    "    with open(gripper_positions_file_path, \"r\") as f:\n",
    "        gripper_positions = json.load(f)\n",
    "\n",
    "    primitives_file_path = os.path.join(args.data_dir, args.libero_task_suite, f\"primitives.json\")\n",
    "    with open(primitives_file_path, \"r\") as f:\n",
    "        primitives = json.load(f)\n",
    "\n",
    "    # reasonings_file_path = os.path.join(args.data_dir, args.libero_task_suite+\"_w_mask\", f\"{args.libero_task_suite}_plan_subtasks.json\")\n",
    "    reasonings_file_path = os.path.join(args.data_dir, args.libero_task_suite, \"filtered_reasoning_h10.json\")\n",
    "    with open(reasonings_file_path, \"r\") as f:\n",
    "        reasonings = json.load(f)\n",
    "\n",
    "    for file_path in tqdm(reasonings.keys(), desc=\"Merging\"):\n",
    "        if file_path not in bboxes:\n",
    "            print(f\"File path {file_path} not found in bboxes\")\n",
    "            continue\n",
    "        if file_path not in gripper_positions:\n",
    "            print(f\"File path {file_path} not found in gripper_positions\")\n",
    "            continue\n",
    "        if file_path not in primitives:\n",
    "            print(f\"File path {file_path} not found in primitives\")\n",
    "            continue\n",
    "        bbox = bboxes[file_path]\n",
    "        gripper_position = gripper_positions[file_path]\n",
    "        primitive = primitives[file_path]\n",
    "\n",
    "        try:\n",
    "            assert len(bbox) == len(gripper_position) == len(primitive) == len(reasonings[file_path][\"0\"][\"reasoning\"]), f\"Length mismatch for {file_path}: {len(bbox)}, {len(gripper_position)}, {len(primitive)}, {len(reasonings[file_path]['0']['reasoning'])}\"\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        reasonings[file_path][\"0\"][\"features\"].update(\n",
    "            {\n",
    "                \"bboxes\": bbox,\n",
    "                \"gripper_position\": gripper_position,\n",
    "                \"move_primitive\": primitive\n",
    "            }\n",
    "        )\n",
    "\n",
    "    target_dir = os.path.join(args.data_dir, args.libero_task_suite, \"data_merged\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(f\"Saving to {target_dir}\")\n",
    "    target_file_path = os.path.join(target_dir, f\"reasoning_{args.libero_task_suite}.json\")\n",
    "\n",
    "    with open(target_file_path, \"w\") as f:\n",
    "        json.dump(reasonings, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelcot1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
